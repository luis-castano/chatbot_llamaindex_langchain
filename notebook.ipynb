{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "from llama_index import LLMPredictor\n",
    "from llama_index import GPTListIndex, GPTSimpleVectorIndex, LangchainEmbedding, PromptHelper, SimpleDirectoryReader\n",
    "\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlanLLM(LLM):\n",
    "    model_name = 'google/flan-t5-base'\n",
    "    pipeline = pipeline(\n",
    "        'text2text-generation',\n",
    "        model=model_name,\n",
    "        # device=0,\n",
    "        model_kwargs={'torch_dtype': torch.bfloat16}\n",
    "    )\n",
    "\n",
    "    def _call(self, prompt, stop=None):\n",
    "        return self.pipeline(prompt, max_length=9999)[0]['generated_text']\n",
    "    \n",
    "    def _identifying_params(self):\n",
    "        return {'name_of_model': self.model_name}\n",
    "    \n",
    "    def _llm_type(self):\n",
    "        return 'custom'\n",
    "    \n",
    "llm_predictor = LLMPredictor(llm=FlanLLM())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/all-mpnet-base-v2\n",
      "Downloading (…)a8e1d/.gitattributes: 100%|██████████| 1.18k/1.18k [00:00<00:00, 1.17MB/s]\n",
      "Downloading (…)_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 190kB/s]\n",
      "Downloading (…)b20bca8e1d/README.md: 100%|██████████| 10.6k/10.6k [00:00<00:00, 10.6MB/s]\n",
      "Downloading (…)0bca8e1d/config.json: 100%|██████████| 571/571 [00:00<00:00, 570kB/s]\n",
      "Downloading (…)ce_transformers.json: 100%|██████████| 116/116 [00:00<00:00, 116kB/s]\n",
      "Downloading (…)e1d/data_config.json: 100%|██████████| 39.3k/39.3k [00:00<00:00, 511kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 438M/438M [00:11<00:00, 38.7MB/s]\n",
      "Downloading (…)nce_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 53.0kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 239/239 [00:00<00:00, 239kB/s]\n",
      "Downloading (…)a8e1d/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 1.46MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 363/363 [00:00<00:00, 363kB/s]\n",
      "Downloading (…)8e1d/train_script.py: 100%|██████████| 13.1k/13.1k [00:00<00:00, 13.1MB/s]\n",
      "Downloading (…)b20bca8e1d/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 987kB/s]\n",
      "Downloading (…)bca8e1d/modules.json: 100%|██████████| 349/349 [00:00<00:00, 349kB/s]\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device: cpu\n"
     ]
    }
   ],
   "source": [
    "hfemb = HuggingFaceEmbeddings()\n",
    "embed_model = LangchainEmbedding(hfemb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = '''\n",
    "The next generation of AI for developers and Google Workspace\n",
    "Mar 14, 2023\n",
    "\n",
    "3 min read\n",
    "\n",
    "We’re bringing the power of generative AI to developers and businesses through Google Cloud and MakerSuite. And we’re introducing new AI-powered features in Google Workspace.\n",
    "\n",
    "Thomas Kurian\n",
    "CEO, Google Cloud\n",
    "Moving lines and dots in the 4 Google colors\n",
    "\n",
    "Google has been investing in AI for many years and bringing its benefits to individuals, businesses and communities. Whether it’s publishing state-of-the-art research, building helpful products or developing tools and resources that enable others, we’re committed to making AI accessible to everyone.\n",
    "\n",
    "We’re now at a pivotal moment in our AI journey. Breakthroughs in generative AI are fundamentally changing how people interact with technology — and at Google, we’ve been responsibly developing large language models so we can safely bring them to our products. Today, we’re excited to share our early progress. Developers and businesses can now try new APIs and products that make it easy, safe and scalable to start building with Google’s best AI models through Google Cloud and a new prototyping environment called MakerSuite. And in Google Workspace, we’re introducing new features that help people harness the power of generative AI to create, connect and collaborate.\n",
    "\n",
    "PaLM API & MakerSuite: An approachable way to explore and prototype with generative AI applications\n",
    "\n",
    "So many technology and platform shifts — from mobile to cloud computing — have inspired entire ecosystems of developers to start new businesses, imagine new products, and transform how they create. We’re in the midst of another shift with AI that is having a profound effect on every industry.\n",
    "\n",
    "For developers who are experimenting with AI, we’re introducing the PaLM API, an easy and safe way to build on top of our best language models. Today, we’re making an efficient model available, in terms of size and capabilities, and we’ll add other sizes soon. The API also comes with an intuitive tool called MakerSuite, which lets you quickly prototype ideas and, over time, will have features for prompt engineering, synthetic data generation and custom-model tuning — all supported by robust safety tools. Select developers can access the PaLM API and MakerSuite in Private Preview today, and stay tuned for our waitlist soon.\n",
    "\n",
    "Bringing generative AI capabilities to Google Cloud\n",
    "\n",
    "For developers who want to build and customize their own models and apps using generative AI, they can access Google’s AI models, including PaLM, on Google Cloud. We’re bringing new generative AI capabilities to our Google Cloud AI portfolio to help developers and organizations access enterprise-level safety, security, privacy, as well as integrate with their existing Cloud solutions:\n",
    "\n",
    "Generative AI support in Vertex AI: Developers and businesses already use Google Cloud’s Vertex AI platform to build and deploy machine learning models and AI applications at scale. We are now providing foundation models, initially for generating text and images, and over time with audio and video. Google Cloud customers will have the ability to discover models, create and modify prompts, fine tune them with their own data, and deploy applications that use these powerful new technologies.\n",
    "\n",
    "Generative AI App Builder: Businesses and governments also want to build their own AI-powered chat interfaces and digital assistants. To enable this, we are introducing Generative AI App Builder, which connects conversational AI flows with out of the box search experiences and foundation models — helping companies build generative AI applications in minutes or hours.\n",
    "\n",
    "New AI partnerships and programs: In addition to announcing new Google Cloud AI products, we’re also committed to being the most open cloud provider. We’re expanding our AI ecosystem and specialized programs for technology partners, AI-focused software providers and startups.\n",
    "\n",
    "Starting today, trusted testers are accessing Vertex AI with Generative AI support and Generative AI App Builder.\n",
    "\n",
    "New generative AI features in Workspace\n",
    "\n",
    "More than 3 billion people already benefit from AI-powered features in Google Workspace, whether it’s using Smart Compose in Gmail or auto-generated summaries in Google Docs. Now, we’re excited to take the next step and bring a limited set of trusted testers a new set of features that makes the process of writing even easier. In Gmail and Google Docs, you can simply type in a topic you’d like to write about, and a draft will be instantly generated for you. So if you’re a manager onboarding a new employee, Workspace saves you the time and effort involved in writing that first welcome email. From there, you can elaborate upon or abbreviate the message or adjust the tone to be more playful or professional — all in just a few clicks. We’ll be rolling out these new experiences to testers in the coming weeks.\n",
    "\n",
    "Generative AI in Docs helping to write a job description.\n",
    "\n",
    "Scaling AI responsibly\n",
    "\n",
    "Generative AI is an emerging and rapidly evolving technology with complex challenges. That’s why we invite external and internal testers to pressure test new experiences, and why we have AI Principles to guide this work. These principles also serve as an ongoing commitment to our customers worldwide who rely on our products to build and grow their businesses safely with AI. Our goal is to continue to be bold and responsible in our approach and partner with others to improve our AI models so they’re safe and helpful for everyone.\n",
    "\n",
    "We’re so excited by the potential of generative AI, and the opportunities it will unlock — from helping people express themselves creatively, to helping developers build brand new types of applications, to transforming how businesses and governments engage their customers and constituents. Stay tuned for more to come in the weeks and months ahead.\n",
    "\n",
    "POSTED IN:\n",
    "AI Google Cloud Google Workspace\n",
    "\n",
    "About the author\n",
    "Thomas Kurian\n",
    "CEO, Google Cloud\n",
    "\n",
    "Thomas joined Google in November 2018 as the CEO of Google Cloud. Prior to Google, Thomas spent 22 years at Oracle, where most recently he was President of Product Development. Before that, Thomas worked at McKinsey as a business analyst and engagement manager. His nearly 30 years of experience have given him a deep knowledge of engineering, enterprise relationships, and leadership of large organizations.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import Document\n",
    "\n",
    "# documents = SimpleDirectoryReader('data').load_data()\n",
    "\n",
    "text_list = [text1]\n",
    "documents = [Document(t) for t in text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set number of output tokens\n",
    "# num_output = 250\n",
    "\n",
    "# # Set maximun input size\n",
    "# max_input_size = 512\n",
    "\n",
    "# # Set maximun chunk overlap\n",
    "# max_chunk_overlap = 20\n",
    "\n",
    "# prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = GPTSimpleVectorIndex(documents, embed_model=embed_model, llm_predictor=llm_predictor)\n",
    "# index = GPTListIndex(documents, embed_model=embed_model, llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n",
    "index = GPTListIndex(documents, embed_model=embed_model, llm_predictor=llm_predictor)\n",
    "\n",
    "# index.save_to_disk('index.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.getLogger().setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thomas Kurian ------------\n"
     ]
    }
   ],
   "source": [
    "response = index.query('Who wrote this document?')\n",
    "# response = index.query('What is this document about?')\n",
    "# response = index.query('What is the most important point in this document?')\n",
    "# response = index.query('Can you summarize this document?')\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index_langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
